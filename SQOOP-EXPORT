
Create table in retail_db with no data (the where cond 1 = 2 is alwasy FALSE, hence the data is not inserted into the newly
created table). No primary key gets created.

create table order_items_export as select * from order_items where 1= 2;

sqoop export \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba \
--password cloudera \
--export-dir /user/cloudera/sqoop/sqoop-import/orderitems/part* \
--table order_items_export \
--m 5 \
--batch

mysql> select count(1) from retail_db.order_items_export;
+----------+
| count(1) |
+----------+
|   172198 |
+----------+
1 row in set (0.39 sec)


run the same command again

The records are inserted again

mysql> select count(1) from retail_db.order_items_export;
+----------+
| count(1) |
+----------+
|   344396 |
+----------+
1 row in set (2.34 sec)



STAGING TABLE CONCEPT: used only in insert mode. works for tables with no primary key
=====================
Create table depts_exp similar to departments table;

mysql> select * from depts_exp;
+---------------+-----------------+
| department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|            10 | Test10          |
|            20 | Test20          |
|            30 | Test30          |
|            40 | Test40          |
+---------------+-----------------+
10 rows in set (0.00 sec)

mysql> describe depts_exp
    -> ;
+-----------------+-------------+------+-----+---------+-------+
| Field           | Type        | Null | Key | Default | Extra |
+-----------------+-------------+------+-----+---------+-------+
| department_id   | int(11)     | NO   |     | 0       |       |
| department_name | varchar(45) | NO   |     | NULL    |       |
+-----------------+-------------+------+-----+---------+-------+
2 rows in set (0.00 sec)

*****


UPDATE MODE (UPDATEONLY):
==========
sqoop export \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba --password cloudera \
--export-dir sqoop/sqoop-export/depts_exp \
--table departments \
--update-mode updateonly \
--update-key department_id 

mysql> select * from departments;
+---------------+-----------------+
| department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|            10 | Test10          |
|            20 | Test20          |
|            30 | Test30          |
|            40 | Test40          |
+---------------+-----------------+
10 rows in set (0.00 sec)

[cloudera@quickstart ~]$ hdfs dfs -cat sqoop/sqoop-export/*
7,FanShop
9,Test09
30,Test 30


mysql> select * from departments;
+---------------+-----------------+
| department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | FanShop         |
|            10 | Test10          |
|            20 | Test20          |
|            30 | Test 30         |
|            40 | Test40          |
+---------------+-----------------+
10 rows in set (0.09 sec)

UPDATE MODE(ALLOWINSERT)
=====================
sqoop export \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba --password cloudera \
--export-dir sqoop/sqoop-export/depts_exp1 \
--table departments \
--update-mode allowinsert \
--update-key department_id 

[cloudera@quickstart ~]$ hdfs dfs -cat sqoop/sqoop-export/depts_exp1
7,Fan Shop
9,Test09
30,Test30

mysql> select * from departments;
+---------------+-----------------+
| department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|             9 | Test09          |
|            10 | Test10          |
|            20 | Test20          |
|            30 | Test30          |
|            40 | Test40          |
+---------------+-----------------+
11 rows in set (0.15 sec)



P.S: When trying to run the command in insert mode, the job failed due to 
duplicate entry for the keys (7,9,30) specified in the export file 

sqoop export \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba --password cloudera \
--export-dir sqoop/sqoop-export/depts_exp1 \
--table departments 


Now, lets try running the same command with an additional new key present in the export file.

[cloudera@quickstart ~]$ hdfs dfs -cat sqoop/sqoop-export/depts_exp2
7,Fan Shop
9,Test09
30,Test30
50,Test50

sqoop export \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba --password cloudera \
--export-dir sqoop/sqoop-export/depts_exp2 \
--table departments 


This also fails but the record is inserted
mysql> select * from departments;
+---------------+-----------------+
| department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|             9 | Test09          |
|            10 | Test10          |
|            20 | Test20          |
|            30 | Test30          |
|            40 | Test40          |
|            50 | Test50          |
+---------------+-----------------+
12 rows in set (0.00 sec)


*********************************************************************************************************************************
*********************************************************************************************************************************
*********************************************************************************************************************************
*********************************************************************************************************************************


Table 27. Common argument

Argument	Description
--connect <jdbc-uri>	Specify JDBC connect string
--connection-manager <class-name>	Specify connection manager class to use
--driver <class-name>	Manually specify JDBC driver class to use
--hadoop-mapred-home <dir>	Override $HADOOP_MAPRED_HOME
--help	Print usage instructions
--password-file	Set path for a file containing the authentication password
-P	Read password from console
--password <password>	Set authentication password
--username <username>	Set authentication username
--verbose	Print more information while working
--connection-param-file <filename>	Optional properties file that provides connection parameters
--relaxed-isolation	Set connection transaction isolation to read uncommitted for the mappers.

Table 28. Validation arguments More Details

Argument	Description
--validate	Enable validation of data copied, supports single table copy only.
--validator <class-name>	Specify validator class to use.
--validation-threshold <class-name>	Specify validation threshold class to use.
--validation-failurehandler <class-name>	Specify validation failure handler class to use.

Table 29. Export control arguments:

Argument	Description
--columns <col,col,colâ€¦>	Columns to export to table
--direct	Use direct export fast path
--export-dir <dir>	HDFS source path for the export
-m,--num-mappers <n>	Use n map tasks to export in parallel
--table <table-name>	Table to populate
--call <stored-proc-name>	Stored Procedure to call
--update-key <col-name>	Anchor column to use for updates. Use a comma separated list of columns if there are more than one column.
--update-mode <mode>	Specify how updates are performed when new rows are found with non-matching keys in database.
Legal values for mode include updateonly (default) and allowinsert.
--input-null-string <null-string>	The string to be interpreted as null for string columns
--input-null-non-string <null-string>	The string to be interpreted as null for non-string columns
--staging-table <staging-table-name>	The table in which data will be staged before being inserted into the destination table.
--clear-staging-table	Indicates that any data present in the staging table can be deleted.
--batch	Use batch mode for underlying statement execution.
