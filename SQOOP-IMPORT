
--https://github.com/dgadiraju/code/blob/a01c909e382ae7da5077486536d58b677c24469f/hadoop/edw/cloudera/sqoop/sqoop_demo.txt



sqoop list-databases \
--connect 'jdbc:mysql://quickstart.cloudera:3306' \
--username retail_dba \
--password cloudera

sqoop list-tables \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba \
--password cloudera 

****************************************************************************************************************************************

sqoop eval \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_dba \
--password cloudera \
--query 'select * from retail_db.departments'


sqoop eval \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--query 'select count(1) from retail_db.departments'


****************************************************************************************************************************************

--table <table-name>	Table to read
===================================
--target-dir <dir>	HDFS destination dir
========================================
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--target-dir=/user/cloudera/sqoop/sqoop-import/default

P.S: Do not create the folder 'default' before executing the command

--as-textfile	      Imports data as plain text (default)
=======================================================
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--as-textfile \
--target-dir=/user/cloudera/sqoop/sqoop-import/txtfil

P.S: Do not create the folder 'txtfil' before executing the command

--as-avrodatafile	  Imports data to Avro Data Files
===================================================
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--as-avrodatafile \
--target-dir=/user/cloudera/sqoop/sqoop-import/avrfil

P.S: Do not create the folder 'avrfil' before executing the command

--as-sequencefile	  Imports data to SequenceFiles
=================================================
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--as-sequencefile \
--target-dir=/user/cloudera/sqoop/sqoop-import/seqfil

P.S: Do not create the folder 'seqfil' before executing the command

--as-parquetfile	  Imports data to Parquet Files
=================================================
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--as-parquetfile \
--target-dir /user/cloudera/sqoop/sqoopprq/prqfil

P.S: Do not create the folder 'prqfil' before executing the command
Target directory should not have any special characters in the name
****************************************************************************************************************************************

If the target directory exists, then add the option --delete-target-dir in the import statement.

sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--as-sequencefile \
--target-dir=/user/cloudera/sqoop/sqoop-import/seqfil
--delete-target-dir


Default mappers = 4. Hence 4 files are created in the target directory.
sqoop import \
--connect 'jdbc:mysql://quickstart.cloudera:3306/retail_db' \
--username retail_db \
--password cloudera \
--table departments \
--target-dir=/user/cloudera/sqoop/sqoop-import/default
--delete-target-dir
--m 1


import-all
compress
boundary value



Table 3. Import control arguments:

Argument	          Description
--append	          Append data to an existing dataset in HDFS

--boundary-query <statement>	Boundary query to use for creating splits
--columns <col,col,colâ€¦>	Columns to import from table
--delete-target-dir	Delete the import target directory if it exists
--direct	Use direct connector if exists for the database
--fetch-size <n>	Number of entries to read from database at once.
--inline-lob-limit <n>	Set the maximum size for an inline LOB
-m,--num-mappers <n>	Use n map tasks to import in parallel
-e,--query <statement>	Import the results of statement.
--split-by <column-name>	Column of the table used to split work units. Cannot be used with --autoreset-to-one-mapper option.
--autoreset-to-one-mapper	Import should use one mapper if a table has no primary key and no split-by column is provided. Cannot be used with --split-by <col> option.
--warehouse-dir <dir>	HDFS parent for table destination
--where <where clause>	WHERE clause to use during import
-z,--compress	Enable compression
--compression-codec <c>	Use Hadoop codec (default gzip)
--null-string <null-string>	The string to be written for a null value for string columns
--null-non-string <null-string>	The string to be written for a null value for non-string columns





