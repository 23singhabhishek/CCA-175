# Reading and Saving Text files (pyspark)
# Each line is read as a record in case of text files
# Files can be read from HDFS, local file system, HDFS with fully qualified names.
# Python is case sensitive

a. Launch pyspark
$ pyspark

b. Spark would be launched

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.6.6 (r266:84292, Jul 23 2015 15:22:56)
SparkContext available as sc, HiveContext available as sqlContext.
>>> from pyspark import SparkContext    (This step is not necessary as we see that SparkContext is availabe as sc)
>>> sc.textFile("departments")

i. hdfs file
   ---------

[cloudera@quickstart ~]$ hdfs dfs -cat departments/par*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data = sc.textFile("departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs1")


ii. hdfs file
    ---------
     
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/par*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data = sc.textFile("/user/cloudera/departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs2")

iii. hdfs file
     ---------


[cloudera@quickstart ~]$ view /etc/hadoop/conf/core-site.xml
.................
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://quickstart.cloudera:8020</value>
    
    .............
    
    
    
>>> data = sc.textFile("hdfs://quickstart.cloudera:8020/user/cloudera/departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs3")

     
iv. local filesystem
    -----------------
[cloudera@quickstart ~]$ cat Documents/Datasets/weathers.csv
id,name
1,FINE
2,OCAST
3,SHWRY
4,


[cloudera@quickstart Documents]$ cat /home/cloudera/Documents/Datasets/weathers.csv
id,name
1,FINE
2,OCAST
3,SHWRY
4,


>>> data = sc.textFile("file:///home/cloudera/Documents/Datasets/weathers.csv")
>>> for i in data.collect():
...   print(i)
...    
id,name
1,FINE
2,OCAST
3,SHWRY
4,

>>> data.saveAsTextFile("pyspark/localds")

*********************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************

# Reading and Saving Sequence files
# metadata driven, key value


*********************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************
Wordcount example:
=================
lines = sc.textFile("pyspark/wordcount")
datsplt = lines.flatMap(lambda a: a.split(" "))
mapvar = datsplt.map(lambda a:(a,1))
wrdcnt = mapvar.reduceByKey(lambda a,b:a+b)

for i in datsplt.take(5):
      print(i)
8
Lessons
in
Love
:

for i in mapvar.take(5):
      print(i)
(u'8', 1)
(u'Lessons', 1)
(u'in', 1)
(u'Love', 1)
(u':', 1)

for i in wrdcnt.take(5):
      print(i)
(u'Lessons', 1)
(u'', 11)
(u'affection,', 1)
(u'help', 2)
(u'happy.', 1)

In the above example, if the transformation map is used on the dataset before using flatMap then the output is as below:
datsplt = lines.map(lambda a: a.split(" "))
for i in datsplt.take(5):
      print(i)

[u'8', u'Lessons', u'in', u'Love', u':']
[u'']
[u'With', u'the', u'help', u'of', u'a', u'few', u'sociologists', u'(and', u"Reader's", u'Digest),', u'author', 
u'Chrisanna', u'Northrup', u'interviewed', u'more', u'than', u'80,000', u'people', u'around', u'the', u'world', u'to', 
u'compile', u'some', u'quantitative', u'data', u'on', u'love,', u'marriage,', u'sex,', u'trust,', u'and', u'more.', u'Here', 
u'are', u'the', u'top', u'lessons', u'learned', u'from', u'the', u'couples', u'who', u'ranked', u'themselves', u'happiest.']
[u'By', u'Brandon', u'Specktor\t']
[u'']

mapvar = datsplt.map(lambda a:(a,1))
for i in mapvar.take(5)
      print(i)

([u'8', u'Lessons', u'in', u'Love', u':'], 1)
([u''], 1)
([u'With', u'the', u'help', u'of', u'a', u'few', u'sociologists', u'(and', u"Reader's", u'Digest),', u'author', u'Chrisanna', u'Northrup', u'interviewed', u'more', u'than', u'80,000', u'people', u'around', u'the', u'world', u'to', u'compile', u'some', u'quantitative', u'data', u'on', u'love,', u'marriage,', u'sex,', u'trust,', u'and', u'more.', u'Here', u'are', u'the', u'top', u'lessons', u'learned', u'from', u'the', u'couples', u'who', u'ranked', u'themselves', u'happiest.'], 1)
([u'By', u'Brandon', u'Specktor\t'], 1)
([u''], 1)

wrdcnt = mapvar.reduceByKey(lambda a,b:a+b)  ==> this function would not work

********************************************************************************************************************************
********************************************************************************************************************************
JOINING DATASETS:
================

mysql> describe retail_db.orders;
+-------------------+-------------+------+-----+---------+----------------+
| Field             | Type        | Null | Key | Default | Extra          |
+-------------------+-------------+------+-----+---------+----------------+
| order_id          | int(11)     | NO   | PRI | NULL    | auto_increment |
| order_date        | datetime    | NO   |     | NULL    |                |
| order_customer_id | int(11)     | NO   |     | NULL    |                |
| order_status      | varchar(45) | NO   |     | NULL    |                |
+-------------------+-------------+------+-----+---------+----------------+
4 rows in set (0.00 sec)

mysql> describe retail_db.order_items;
+--------------------------+------------+------+-----+---------+----------------+
| Field                    | Type       | Null | Key | Default | Extra          |
+--------------------------+------------+------+-----+---------+----------------+
| order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment |
| order_item_order_id      | int(11)    | NO   |     | NULL    |                |
| order_item_product_id    | int(11)    | NO   |     | NULL    |                |
| order_item_quantity      | tinyint(4) | NO   |     | NULL    |                |
| order_item_subtotal      | float      | NO   |     | NULL    |                |
| order_item_product_price | float      | NO   |     | NULL    |                |
+--------------------------+------------+------+-----+---------+----------------+
6 rows in set (0.00 sec)

# Problem statement, get the revenue and number of orders from order_items on daily basis

select count(distinct(a.order_id)), a.order_date, sum(b.order_item_subtotal) 
from orders a, order_items b where a.order_id = b.order_item_order_id group by a.order_date 

a. Load the data
   -------------
ordRDD = sc.textFile("sqoop/sqoop-import/orders")

orditmsRDD = sc.textFile("sqoop/sqoop-import/order_items")

b. Extract the required fields (orders table: order_id & order_date, 
   -----------------------------------------------------------------
order_items table: order_item_order_id & order_item_subtotal)
------------------------------------------------------------
datsplt1 = ordRDD.map(lambda a: (a.split(",")[0], a.split(",")[1]))

The order_id field is extracted as string. It is converted to int.

datsplt1 = ordRDD.map(lambda a: (int(a.split(",")[0]), a.split(",")[1]))

sample data:
-----------
(1, u'2013-07-25 00:00:00.0')
(2, u'2013-07-25 00:00:00.0')
(3, u'2013-07-25 00:00:00.0')
(4, u'2013-07-25 00:00:00.0')
(5, u'2013-07-25 00:00:00.0')
(6, u'2013-07-25 00:00:00.0')
(7, u'2013-07-25 00:00:00.0')
(8, u'2013-07-25 00:00:00.0')
(9, u'2013-07-25 00:00:00.0')


datsplt2 = orditmsRDD.map(lambda a: (int(a.split(",")[1]), float(a.split(",")[4])))
sample data:
-----------
(1, 299.98000000000002)
(2, 199.99000000000001)
(2, 250.0)
(2, 129.99000000000001)
(4, 49.979999999999997)
(4, 299.94999999999999)
(4, 150.0)

c. Datasets are joined using order_id as the joining key
   -----------------------------------------------------
join1 = datsplt2.join(datsplt1)
(1, (299.98000000000002, u'2013-07-25 00:00:00.0'))
(2, (199.99000000000001, u'2013-07-25 00:00:00.0'))
(2, (250.0, u'2013-07-25 00:00:00.0'))
(2, (129.99000000000001, u'2013-07-25 00:00:00.0'))
(4, (49.979999999999997, u'2013-07-25 00:00:00.0'))
(4, (299.94999999999999, u'2013-07-25 00:00:00.0'))
(4, (150.0, u'2013-07-25 00:00:00.0'))
(4, (199.91999999999999, u'2013-07-25 00:00:00.0'))

c. Extract the date and subtotal details
   -------------------------------------
joinrev = join1.map(lambda a: (a[1][1],a[1][0]))
(u'2014-02-12 00:00:00.0', 199.99000000000001)
(u'2014-02-12 00:00:00.0', 129.99000000000001)
(u'2014-02-12 00:00:00.0', 299.98000000000002)
(u'2014-02-12 00:00:00.0', 399.98000000000002)
(u'2014-05-27 00:00:00.0', 299.98000000000002)
(u'2013-07-25 00:00:00.0', 179.97)
(u'2013-07-25 00:00:00.0', 299.94999999999999)
(u'2013-07-25 00:00:00.0', 199.91999999999999)
(u'2013-07-25 00:00:00.0', 50.0)
(u'2014-06-19 00:00:00.0', 199.97999999999999)
(u'2014-06-19 00:00:00.0', 249.90000000000001)

d. Calculate the revenue per day
   -----------------------------
revperday = joinrev.reduceByKey(lambda a,b: a + b)
(u'2013-08-23 00:00:00.0', 99616.169999999925)
(u'2014-04-06 00:00:00.0', 56192.119999999981)
(u'2013-11-29 00:00:00.0', 136296.62999999995)
(u'2014-03-18 00:00:00.0', 122522.78999999994)
(u'2014-05-09 00:00:00.0', 125145.24999999994)

e. Sort the dataset based on order_date
   ------------------------------------
revsort = revperday.sortByKey()
(u'2013-07-25 00:00:00.0', 68153.829999999973)
(u'2013-07-26 00:00:00.0', 136520.16999999993)
(u'2013-07-27 00:00:00.0', 101074.33999999992)
(u'2013-07-28 00:00:00.0', 87123.079999999958)
(u'2013-07-29 00:00:00.0', 137287.08999999991)
(u'2013-07-30 00:00:00.0', 102745.61999999994)
(u'2013-07-31 00:00:00.0', 131878.05999999991)
(u'2013-08-01 00:00:00.0', 129001.61999999989)

f. Extract order_Date and order_id
   -------------------------------
joinord = join1.map(lambda a: (a[1][1], a[0]))
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 16)
(u'2013-07-25 00:00:00.0', 16)
(u'2013-07-25 00:00:00.0', 24)
(u'2013-07-25 00:00:00.0', 24)
(u'2013-07-25 00:00:00.0', 24)

g. get distinct order_ids
   ----------------------
orddist = joinord.distinct()

h. Assign value 1 to each order_date
   ---------------------------------
joinord1 = orddist.map(lambda a: (a[0],1))
(u'2014-02-12 00:00:00.0', 1)
(u'2014-02-12 00:00:00.0', 1)
(u'2014-02-12 00:00:00.0', 1)
(u'2014-02-12 00:00:00.0', 1)
(u'2014-05-27 00:00:00.0', 1)
(u'2013-07-25 00:00:00.0', 1)

i. Calculate total number of orders per day
   ----------------------------------------
ordperday = joinord1.reduceByKey(lambda a,b: a + b)
(u'2013-08-23 00:00:00.0', 169)
(u'2014-04-06 00:00:00.0', 85)
(u'2014-07-14 00:00:00.0', 137)
(u'2014-03-18 00:00:00.0', 208)
(u'2014-05-09 00:00:00.0', 207)
(u'2013-09-02 00:00:00.0', 162)

j. Join the revenue per day and order per day details
   --------------------------------------------------
finalds = ordperday.join(revperday)
finaldssrt = finalds.sortByKey()

(u'2013-07-25 00:00:00.0', (116, 68153.829999999973))
(u'2013-07-26 00:00:00.0', (233, 136520.16999999993))
(u'2013-07-27 00:00:00.0', (175, 101074.33999999992))
(u'2013-07-28 00:00:00.0', (158, 87123.079999999958))
(u'2013-07-29 00:00:00.0', (216, 137287.08999999991))
(u'2013-07-30 00:00:00.0', (182, 102745.61999999994))
(u'2013-07-31 00:00:00.0', (209, 131878.05999999991))
(u'2013-08-01 00:00:00.0', (212, 129001.61999999989))





********************************************************************************************************************************
********************************************************************************************************************************

leftOuterJoin
rightOuterJoin
fullOuterJoin

********************************************************************************************************************************
********************************************************************************************************************************


********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************
