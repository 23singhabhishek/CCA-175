# Reading and Saving Text files (pyspark)
# Each line is read as a record in case of text files
# Files can be read from HDFS, local file system, HDFS with fully qualified names.
# Python is case sensitive

a. Launch pyspark
$ pyspark

b. Spark would be launched

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Python version 2.6.6 (r266:84292, Jul 23 2015 15:22:56)
SparkContext available as sc, HiveContext available as sqlContext.
>>> from pyspark import SparkContext    (This step is not necessary as we see that SparkContext is availabe as sc)
>>> sc.textFile("departments")

i. hdfs file
   ---------

[cloudera@quickstart ~]$ hdfs dfs -cat departments/par*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data = sc.textFile("departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs1")


ii. hdfs file
    ---------
     
[cloudera@quickstart ~]$ hdfs dfs -cat /user/cloudera/departments/par*
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data = sc.textFile("/user/cloudera/departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs2")

iii. hdfs file
     ---------


[cloudera@quickstart ~]$ view /etc/hadoop/conf/core-site.xml
.................
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://quickstart.cloudera:8020</value>
    
    .............
    
    
    
>>> data = sc.textFile("hdfs://quickstart.cloudera:8020/user/cloudera/departments")
>>> for i in data.collect():
...   print(i)
...
2,Fitness
3,Footwear
4,Apparel
5,Golf
6,Outdoors
7,Fan Shop

>>> data.saveAsTextFile("pyspark/hdfs3")

     
iv. local filesystem
    -----------------
[cloudera@quickstart ~]$ cat Documents/Datasets/weathers.csv
id,name
1,FINE
2,OCAST
3,SHWRY
4,


[cloudera@quickstart Documents]$ cat /home/cloudera/Documents/Datasets/weathers.csv
id,name
1,FINE
2,OCAST
3,SHWRY
4,


>>> data = sc.textFile("file:///home/cloudera/Documents/Datasets/weathers.csv")
>>> for i in data.collect():
...   print(i)
...    
id,name
1,FINE
2,OCAST
3,SHWRY
4,

>>> data.saveAsTextFile("pyspark/localds")

*********************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************

# Reading and Saving Sequence files
# metadata driven, key value


*********************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************
Wordcount example:
=================
lines = sc.textFile("pyspark/wordcount")
datsplt = lines.flatMap(lambda a: a.split(" "))
mapvar = datsplt.map(lambda a:(a,1))
wrdcnt = mapvar.reduceByKey(lambda a,b:a+b)

for i in datsplt.take(5):
      print(i)
8
Lessons
in
Love
:

for i in mapvar.take(5):
      print(i)
(u'8', 1)
(u'Lessons', 1)
(u'in', 1)
(u'Love', 1)
(u':', 1)

for i in wrdcnt.take(5):
      print(i)
(u'Lessons', 1)
(u'', 11)
(u'affection,', 1)
(u'help', 2)
(u'happy.', 1)

In the above example, if the transformation map is used on the dataset before using flatMap then the output is as below:
datsplt = lines.map(lambda a: a.split(" "))
for i in datsplt.take(5):
      print(i)

[u'8', u'Lessons', u'in', u'Love', u':']
[u'']
[u'With', u'the', u'help', u'of', u'a', u'few', u'sociologists', u'(and', u"Reader's", u'Digest),', u'author', 
u'Chrisanna', u'Northrup', u'interviewed', u'more', u'than', u'80,000', u'people', u'around', u'the', u'world', u'to', 
u'compile', u'some', u'quantitative', u'data', u'on', u'love,', u'marriage,', u'sex,', u'trust,', u'and', u'more.', u'Here', 
u'are', u'the', u'top', u'lessons', u'learned', u'from', u'the', u'couples', u'who', u'ranked', u'themselves', u'happiest.']
[u'By', u'Brandon', u'Specktor\t']
[u'']

mapvar = datsplt.map(lambda a:(a,1))
for i in mapvar.take(5)
      print(i)

([u'8', u'Lessons', u'in', u'Love', u':'], 1)
([u''], 1)
([u'With', u'the', u'help', u'of', u'a', u'few', u'sociologists', u'(and', u"Reader's", u'Digest),', u'author', u'Chrisanna', u'Northrup', u'interviewed', u'more', u'than', u'80,000', u'people', u'around', u'the', u'world', u'to', u'compile', u'some', u'quantitative', u'data', u'on', u'love,', u'marriage,', u'sex,', u'trust,', u'and', u'more.', u'Here', u'are', u'the', u'top', u'lessons', u'learned', u'from', u'the', u'couples', u'who', u'ranked', u'themselves', u'happiest.'], 1)
([u'By', u'Brandon', u'Specktor\t'], 1)
([u''], 1)

wrdcnt = mapvar.reduceByKey(lambda a,b:a+b)  ==> this function would not work

********************************************************************************************************************************
********************************************************************************************************************************
JOINING DATASETS:
================

mysql> describe retail_db.orders;
+-------------------+-------------+------+-----+---------+----------------+
| Field             | Type        | Null | Key | Default | Extra          |
+-------------------+-------------+------+-----+---------+----------------+
| order_id          | int(11)     | NO   | PRI | NULL    | auto_increment |
| order_date        | datetime    | NO   |     | NULL    |                |
| order_customer_id | int(11)     | NO   |     | NULL    |                |
| order_status      | varchar(45) | NO   |     | NULL    |                |
+-------------------+-------------+------+-----+---------+----------------+
4 rows in set (0.00 sec)

mysql> describe retail_db.order_items;
+--------------------------+------------+------+-----+---------+----------------+
| Field                    | Type       | Null | Key | Default | Extra          |
+--------------------------+------------+------+-----+---------+----------------+
| order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment |
| order_item_order_id      | int(11)    | NO   |     | NULL    |                |
| order_item_product_id    | int(11)    | NO   |     | NULL    |                |
| order_item_quantity      | tinyint(4) | NO   |     | NULL    |                |
| order_item_subtotal      | float      | NO   |     | NULL    |                |
| order_item_product_price | float      | NO   |     | NULL    |                |
+--------------------------+------------+------+-----+---------+----------------+
6 rows in set (0.00 sec)

# Problem statement, get the revenue and number of orders from order_items on daily basis

select count(distinct(a.order_id)), a.order_date, sum(b.order_item_subtotal) 
from orders a, order_items b where a.order_id = b.order_item_order_id group by a.order_date ;
364 rows

mysql> select count(distinct(order_item_order_id)) from order_items;
57431 rows




a. Load the data
   -------------
ordRDD = sc.textFile("sqoop/sqoop-import/orders")

orditmsRDD = sc.textFile("sqoop/sqoop-import/order_items")

b. Extract the required fields (orders table: order_id & order_date, 
   -----------------------------------------------------------------
order_items table: order_item_order_id & order_item_subtotal)
------------------------------------------------------------
datsplt1 = ordRDD.map(lambda a: (a.split(",")[0], a.split(",")[1]))

The order_id field is extracted as string. It is converted to int.

datsplt1 = ordRDD.map(lambda a: (int(a.split(",")[0]), a.split(",")[1]))
sample data:
-----------
(1, u'2013-07-25 00:00:00.0')
(2, u'2013-07-25 00:00:00.0')
(3, u'2013-07-25 00:00:00.0')
(4, u'2013-07-25 00:00:00.0')


datsplt2 = orditmsRDD.map(lambda a: (int(a.split(",")[1]), float(a.split(",")[4])))
sample data:
-----------
(1, 299.98000000000002)
(2, 199.99000000000001)
(2, 250.0)
(2, 129.99000000000001)
(4, 49.979999999999997)

datsplt2rnd = datsplt2.map(lambda a:(a[0], round(a[1],2)))  (****need to check on how to round a floating point number)

c. Datasets are joined using order_id as the joining key
   -----------------------------------------------------
join1 = datsplt2.join(datsplt1)
(1, (299.98000000000002, u'2013-07-25 00:00:00.0'))
(2, (199.99000000000001, u'2013-07-25 00:00:00.0'))
(2, (250.0, u'2013-07-25 00:00:00.0'))
(2, (129.99000000000001, u'2013-07-25 00:00:00.0'))
(4, (49.979999999999997, u'2013-07-25 00:00:00.0'))


c. Extract the date and subtotal details
   -------------------------------------
joinrev = join1.map(lambda a: (a[1][1],a[1][0]))
(u'2013-07-25 00:00:00.0', 179.97)
(u'2013-07-25 00:00:00.0', 299.94999999999999)
(u'2013-07-25 00:00:00.0', 199.91999999999999)
(u'2013-07-25 00:00:00.0', 50.0)
(u'2013-07-25 00:00:00.0', 119.98)


d. Calculate the revenue per day
   -----------------------------
revperday = joinrev.reduceByKey(lambda a,b: a + b)
(u'2013-07-25 00:00:00.0', 68153.829999999973)
(u'2013-07-26 00:00:00.0', 136520.16999999993)
(u'2013-07-27 00:00:00.0', 101074.33999999992)
(u'2013-07-28 00:00:00.0', 87123.079999999958)
(u'2013-07-29 00:00:00.0', 137287.08999999991)

e. Sort the dataset based on order_date  (Optional step)
   ------------------------------------
revsort = revperday.sortByKey()
(u'2013-07-25 00:00:00.0', 68153.829999999973)
(u'2013-07-26 00:00:00.0', 136520.16999999993)
(u'2013-07-27 00:00:00.0', 101074.33999999992)
(u'2013-07-28 00:00:00.0', 87123.079999999958)

f. Extract order_Date and order_id
   -------------------------------
joinord = join1.map(lambda a: (a[1][1], a[0]))
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 8)
(u'2013-07-25 00:00:00.0', 16)

g. get distinct order_ids
   ----------------------
orddist = joinord.distinct()

(u'2013-07-25 00:00:00.0', 5)
(u'2013-07-25 00:00:00.0', 45)
(u'2013-07-25 00:00:00.0', 57773)
(u'2013-07-25 00:00:00.0', 57781)
(u'2013-07-25 00:00:00.0', 29)

h. Assign value 1 to each order_date
   ---------------------------------
joinord1 = orddist.map(lambda a: (a[0],1))
(u'2013-07-25 00:00:00.0', 1)
(u'2013-07-25 00:00:00.0', 1)
(u'2013-07-25 00:00:00.0', 1)
(u'2013-07-25 00:00:00.0', 1)
(u'2013-07-25 00:00:00.0', 1)


i. Calculate total number of orders per day
   ----------------------------------------
ordperday = joinord1.reduceByKey(lambda a,b: a + b)
(u'2013-07-25 00:00:00.0', 116)
(u'2013-07-26 00:00:00.0', 233)
(u'2013-07-27 00:00:00.0', 175)
(u'2013-07-28 00:00:00.0', 158)
(u'2013-07-29 00:00:00.0', 216)

j. Join the revenue per day and order per day details
   --------------------------------------------------
finalds = ordperday.join(revperday)
finaldssrt = finalds.sortByKey()

(u'2013-07-25 00:00:00.0', (116, 68153.829999999973))
(u'2013-07-26 00:00:00.0', (233, 136520.16999999993))
(u'2013-07-27 00:00:00.0', (175, 101074.33999999992))
(u'2013-07-28 00:00:00.0', (158, 87123.079999999958))
(u'2013-07-29 00:00:00.0', (216, 137287.08999999991))

Count: 364 records




********************************************************************************************************************************
********************************************************************************************************************************

leftOuterJoin
rightOuterJoin
fullOuterJoin

********************************************************************************************************************************
********************************************************************************************************************************
JOINING DATASETS USING SQL
==========================
HiveContext:
the tables should already be created in Hive

from pyspark.sql import HiveContext
sqlc = HiveContext(sc)
sqlc.sql("set spark.sql.shuffle.partitions =10")
sqlc.sql("select count(1) from sqoop.orders")

cntord = sqlc.sql("select count(1) from sqoop.orders")
68883
cntorditm = sqlc.sql("select count(1) from sqoop.order_items")
172198

joinRDD = sqlc.sql("select a.order_date, count(distinct(a.order_id)), round(sum(b.order_item_subtotal),2) from sqoop.orders a join  \
sqoop.order_items b on a.order_id = b.order_item_order_id group by a.order_date order by a.order_date")
joinRDD.count ==> 364
for i in joinRDD.take(10):
      print(i)
      
Row(order_date=u'2013-07-25 00:00:00.0', _c1=116, _c2=68153.830000000002)
Row(order_date=u'2013-07-26 00:00:00.0', _c1=233, _c2=136520.17000000001)
Row(order_date=u'2013-07-27 00:00:00.0', _c1=175, _c2=101074.34)
Row(order_date=u'2013-07-28 00:00:00.0', _c1=158, _c2=87123.080000000002)
Row(order_date=u'2013-07-29 00:00:00.0', _c1=216, _c2=137287.09)
Row(order_date=u'2013-07-30 00:00:00.0', _c1=182, _c2=102745.62)
Row(order_date=u'2013-07-31 00:00:00.0', _c1=209, _c2=131878.06)
Row(order_date=u'2013-08-01 00:00:00.0', _c1=212, _c2=129001.62)
Row(order_date=u'2013-08-02 00:00:00.0', _c1=186, _c2=109347.0)
Row(order_date=u'2013-08-03 00:00:00.0', _c1=159, _c2=95266.889999999999)

SqlContext:
==========

from pyspark.sql import SQLContext, Row
sqlc = SQLContext(sc)
sqlc.sql("set spark.sql.shuffle.partitions=10")    (o/p: DataFrame[key: string, value: string])******


ordRDD = sc.textFile("sqoop/sqoop-import/orders")
map1 = ordRDD.map(lambda a: a.split(","))
orders = map1.map(lambda a: Row(order_id = int(a[0]), order_date = a[1], order_customer_id = int(a[2]), order_status = a[3]))
ordersSchema = sqlc.inferSchema(orders)
ordersSchema.registerTempTable("orders")

orditRDD = sc.textFile("sqoop/sqoop-import/order_items")
map2 = orditRDD.map(lambda a:a.split(","))
order_items = map2.map(lambda a: Row(order_item_id = int(a[0]), order_item_order_id = int(a[1]), \
order_item_product_id = int(a[2]), order_item_quantity = int(a[3]), order_item_subtotal = float(a[4]), \
product_price = float(a[5])))
ord_itmsSchema = sqlc.inferSchema(order_items)
ord_itmsSchema.registerTempTable("order_items")

joinRDD = sqlc.sql("select a.order_date, count(distinct(a.order_id)), sum(b.order_item_subtotal) from orders a join  \
order_items b on a.order_id = b.order_item_order_id group by a.order_date order by a.order_date")

Check for float in Sql context
for i in joinRDD.take(10):
      print(i)     

Row(order_date=u'2013-07-25 00:00:00.0', _c1=116, _c2=68153.829999999973)
Row(order_date=u'2013-07-26 00:00:00.0', _c1=233, _c2=136520.1699999999)
Row(order_date=u'2013-07-27 00:00:00.0', _c1=175, _c2=101074.33999999995)
Row(order_date=u'2013-07-28 00:00:00.0', _c1=158, _c2=87123.079999999958)
Row(order_date=u'2013-07-29 00:00:00.0', _c1=216, _c2=137287.08999999991)
Row(order_date=u'2013-07-30 00:00:00.0', _c1=182, _c2=102745.61999999994)
Row(order_date=u'2013-07-31 00:00:00.0', _c1=209, _c2=131878.05999999994)
Row(order_date=u'2013-08-01 00:00:00.0', _c1=212, _c2=129001.61999999991)
Row(order_date=u'2013-08-02 00:00:00.0', _c1=186, _c2=109346.99999999994)
Row(order_date=u'2013-08-03 00:00:00.0', _c1=159, _c2=95266.889999999956)

joinRDDcount --> 364
********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************

********************************************************************************************************************************
********************************************************************************************************************************
